{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg \n",
    "moby_raw = gutenberg.raw('melville-moby_dick.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_one():\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    return len(word_tokenize(moby_raw)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255,028\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_one()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_two():    \n",
    "    return len(set(nltk.word_tokenize(moby_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20,742\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_two()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/maks/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in nltk.word_tokenize(moby_raw)]\n",
    "    return len(set(lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16,887\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_three()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Diversity: 0.08133224587104161\n"
     ]
    }
   ],
   "source": [
    "def answer_one():\n",
    "    total_tokens = example_one()\n",
    "    unique_tokens = example_two()\n",
    "    lexical_diversity = unique_tokens / total_tokens\n",
    "    return lexical_diversity\n",
    "\n",
    "print('Lexical Diversity:', answer_one())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def answer_two():\n",
    "    tokens = word_tokenize(moby_raw)\n",
    "    total_tokens = len(tokens)\n",
    "    whale_count = sum(1 for token in tokens if token.lower() == 'whale')\n",
    "    Whale_count = sum(1 for token in tokens if token == 'Whale')\n",
    "    total_whale_count = whale_count + Whale_count\n",
    "    percentage = (total_whale_count / total_tokens) * 100\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5317063224430258"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequently occurring tokens: [(',', 19204), ('the', 13715), ('.', 7306), ('of', 6513), ('and', 6010), ('a', 4545), ('to', 4515), (';', 4173), ('in', 3908), ('that', 2978), ('his', 2459), ('it', 2196), ('I', 2113), ('!', 1767), ('is', 1722), ('--', 1713), ('with', 1659), ('he', 1658), ('was', 1639), ('as', 1620)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def answer_three():\n",
    "    tokens = word_tokenize(moby_raw)\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    most_common = freq_dist.most_common(20)\n",
    "    return most_common\n",
    "\n",
    "print('Top 20 most frequently occurring tokens:', answer_three())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with length > 5 and frequency > 150: ['Captain', 'Pequod', 'Queequeg', 'Starbuck', 'almost', 'before', 'himself', 'little', 'seemed', 'should', 'though', 'through', 'whales', 'without']\n"
     ]
    }
   ],
   "source": [
    "def answer_four():\n",
    "    tokens = word_tokenize(moby_raw)\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    filtered_tokens = [token for token, freq in freq_dist.items() if len(token) > 5 and freq > 150]\n",
    "    sorted_filtered_tokens = sorted(filtered_tokens)\n",
    "    return sorted_filtered_tokens\n",
    "\n",
    "print('Tokens with length > 5 and frequency > 150:', answer_four())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word and its length: (\"twelve-o'clock-at-night\", 23)\n"
     ]
    }
   ],
   "source": [
    "def answer_five():\n",
    "    tokens = word_tokenize(moby_raw)\n",
    "    longest_word = max(tokens, key=len)\n",
    "    return longest_word, len(longest_word)\n",
    "\n",
    "print('Longest word and its length:', answer_five())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words with frequency > 2000: [('the', 13715), ('of', 6513), ('and', 6010), ('a', 4545), ('to', 4515), ('in', 3908), ('that', 2978), ('his', 2459), ('it', 2196), ('I', 2113)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def answer_six():\n",
    "    tokens = word_tokenize(moby_raw)\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    filtered_words = [(word, freq) for word, freq in freq_dist.items() if freq > 2000 and re.match(\"^[a-zA-Z0-9]+$\", word)]\n",
    "    sorted_filtered_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_filtered_words\n",
    "\n",
    "print('Unique words with frequency > 2000:', answer_six())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 25.88591149005278\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def answer_seven():\n",
    "    sentences = sent_tokenize(moby_raw)\n",
    "    num_tokens_per_sentence = [len(word_tokenize(sentence)) for sentence in sentences]\n",
    "    average_tokens_per_sentence = sum(num_tokens_per_sentence) / len(num_tokens_per_sentence)\n",
    "    return average_tokens_per_sentence\n",
    "\n",
    "print('Average number of tokens per sentence:', answer_seven())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most frequent parts of speech: [('NN', 32727), ('IN', 28662), ('DT', 25879), (',', 19204), ('JJ', 17613)]\n"
     ]
    }
   ],
   "source": [
    "def answer_eight():\n",
    "    tokens = nltk.word_tokenize(moby_raw)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    pos_counts = {}\n",
    "    \n",
    "    for word, tag in pos_tags:\n",
    "        if tag in pos_counts:\n",
    "            pos_counts[tag] += 1\n",
    "        else:\n",
    "            pos_counts[tag] = 1\n",
    "    \n",
    "    pos_counts_list = [(tag, count) for tag, count in pos_counts.items()]\n",
    "    \n",
    "    sorted_pos_counts = sorted(pos_counts_list, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_pos_counts[:5]\n",
    "\n",
    "print('Top 5 most frequent parts of speech:', answer_eight())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corpulent', 'intendence', 'validate']\n"
     ]
    }
   ],
   "source": [
    "def answer_nine(default_words=['cormulent', 'incendenece', 'validrate']):\n",
    "    correct_spellings = nltk.corpus.words.words()\n",
    "    recommendations = []\n",
    "    \n",
    "    for word in default_words:\n",
    "        candidates = [w for w in correct_spellings if w.startswith(word[0])]\n",
    "        \n",
    "        edit_distances = [(w, nltk.edit_distance(word, w, transpositions=True)) for w in candidates]\n",
    "\n",
    "        recommendation = min(edit_distances, key=lambda x: x[1])[0]\n",
    "        \n",
    "        recommendations.append(recommendation)\n",
    "    \n",
    "    return [word for word in recommendations]\n",
    "\n",
    "print(answer_nine(['cormulent', 'incendenece', 'validrate']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
